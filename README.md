# C^3-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking


<p align="center">
    üìñ <a>English</a> ‚Ä¢
    <a href="README_ZH.md">‰∏≠Êñá</a>
</p>


![Example](./picture/first.png)


## üìñ Overview

Agents based on large language models leverage tools to modify environments, revolutionizing how AI interacts with the physical world. Unlike traditional NLP tasks that rely solely on historical dialogue for responses, these agents must consider more complex factors, such as inter-tool relationships, environmental feedback and previous decisions, when making choices.
Current research typically evaluates agents via multi-turn dialogues. However, it overlooks the influence of these critical factors on agent behavior.
To bridge this gap, we present an open-source and high-quality benchmark C^3-Bench.
This benchmark integrates attack concepts and applies univariate analysis to pinpoint key elements affecting agent robustness.
In concrete, we design three challenges: navigate complex tool relationships, handle critical hidden information and manage dynamic decision paths.
Complementing these challenges, we introduce fine-grained metrics, innovative data collection algorithms and reproducible evaluation methods.
Extensive experiments are conducted on 49 mainstream agents, encompassing general fast-thinking, slow-thinking and domain-specific models.
We observe that agents have significant shortcomings in handling tool dependencies, long context information dependencies and frequent policy-type switching.
In essence, C^3-Bench aims to expose model vulnerabilities through these challenges and drive research into the interpretability of agent performance.


## üòä Key Materials

- Test data location: c3_bench/data/C3-Bench.jsonl
- Location of the prediction results for the 49 models reported in the paper: c3_bench/bench_test/result
- More detailed information about the C3-Bench can be found below

## ‚ö°Ô∏è Quickstart

### Basic Installation

```bash
# Create a new Conda environment with Python 3.10
conda create -n C3-Bench python=3.10
conda activate C3-Bench

# Clone the C3-Bench repository
git clone https://anonymous.4open.science/r/C3-Bench-Anonymous-38D9.git

# Change directory to the `c3_bench`
cd c3_bench/

# Install the package
pip install -r requirements.txt
```

## ‚è≥ Inference

### üíæ Test Data

![overall](./picture/example.png)

Address: c3_bench/data/C3-Bench.jsonl

Description: Our test data has undergone five rounds of manual inspection and correction by five senior algorithm researcher with years of experience in NLP, CV, and LLM, taking about one month in total. It boasts extremely high quality and accuracy, with a tight connection between multiple rounds of tasks, increasing difficulty, no unusable invalid data, and complete consistency with human distribution. Its evaluation results and conclusions are of great reference value for subsequent optimization in the Agent direction.

Specifically, the data quality optimization work went through the following stages:

1. The initial data was generated using our proposed Multi Agent Data Generation framework, covering all possible action spaces.

2. The test data was then divided according to four different types of actions defined by us and manually inspected and corrected by four different algorithm researcher. Specifically, since tasks generated by LLM are always too formal and not colloquial enough, especially after the second task, it is difficult to generate true multi-turn tasks. Therefore, we conducted the first round of corrections based on the criteria of colloquialism and true multi-turn tasks. Notably, in designing the third and fourth round tasks, we added tasks with long-term memory, a true multi-turn type, to increase the difficulty of the test set.

Note: In the actual construction process, the four algorithm researcher adopted a layer-by-layer approach, first generating a layer of data with the model, then manually inspecting and correcting it, before generating and correcting the next layer of data. This approach avoids the difficulty of ensuring overall correctness and maintaining data coherence when, after generating all layers of data at once, a problem in one layer requires corrections that often affect both the previous and subsequent layers. Thus, our layer-by-layer construction ensures strong logical consistency and close relationships between layers, without any unreasonable trajectories.

3. After the first round of corrections by the four algorithm researcher, one senior experts in the Agent field would comment on each piece of data, indicating whether it meets the requirements and what problems exist, followed by a second correction by the four algorithm researcher.

4. After the second round of corrections, we introduced cross-validation, where the four algorithm researcher inspected and commented on each other's data. Then, the four algorithm researcher and one senior experts in the Agent field discussed and made a third round of corrections on the doubtful data.

5. After the third round of corrections, the one senior experts in the Agent field separately conducted a fourth round of inspection and correction on all data to ensure absolute accuracy.

6. Finally, since human corrections might introduce errors, we used code to check for possible parameter type errors and unreasonable dependencies caused by manual operations, with one senior experts making the final fifth round of corrections.

Through these five stages of data quality optimization, each piece of data was manually corrected and constructed by multiple algorithm experts, improving our test data's accuracy from less than 60% initially to 100% correctness. The combination of model generation and multiple human corrections also endowed our data with excellent diversity and quality. 

At the same time, compared to other benchmarks such as BFCL, T-EVAL, etc., our test data covers all possible action spaces, and in the second to fourth rounds of true multi-turn tasks, the coverage rate has reached two 100%, which also makes our data distribution very balanced, capable of testing out the weaknesses of the model without any blind spots.

![compare](./picture/compare.png)

Ultimately, this high-quality data set we constructed laid the foundation for our subsequent experiments, lending absolute credibility to our conclusions.

Additionally, we provide bilingual support for the test data, including both English and Chinese versions, all of which have undergone the aforementioned manual inspection process. Subsequent LeadBoard results will primarily report the English version.

## üõ†Ô∏è Framework

Our evaluation framework is constructed in a manner that separates inference and results analysis, offering several advantages as follows:

- High reproducibility: In our test data, the execution results of all tools corresponding to the golden answers have been persistently saved. There is no need for any website's KEY, and there are no unstable tool invocation scenarios, ensuring the reproducibility of the results.
- High evaluation efficiency: Our evaluation is conducted dynamically. The first phase is carried out using EvalByToolCallGraph module, deciding whether to continue calling based on whether the action (predicted tool name) matches the golden answer. Meanwhile, decision tree pruning is used during the process, significantly reducing the number of maintenance paths and speeding up the evaluation.
- High code reusability: All our requests use the standard ToolCalls protocol, making our evaluation code highly reusable. Additionally, we have encapsulated the ToolCalls protocol for several open-source general models and open-source specialized models that did not support the ToolCalls protocol, making the code logic clearer and solving the problem of other evaluation frameworks mixing Prompt and ToolCalls invocation methods, leading to confused logic.
- Multiple evaluation analysis dimensions: After obtaining the prediction and action-level evaluation results from the first phase, we use the AnalysisResult module to conduct a detailed evaluation of its results, including analyses across six dimensions. To our knowledge, among all Agent evaluation frameworks, we offer the most analysis dimensions and the most detailed results. Also, our results are saved in CSV files, facilitating developers in bad case analysis.
- Strong scalability: Since we use the standard ToolCalls protocol, for API models, GPTHandle can be used for rapid integration; for new open-source models, we will continue to update this repository for integration; for developers' own trained models, they can refer to our Handle code to encapsulate the Prompt calling method into the ToolCalls protocol for rapid integration and verification.

The overall framework diagram is as follows:

![Framework](./picture/framework.png)


### ü§ñ API Models
This project supports multiple API models, including: GPT-4o„ÄÅGPT-4.1„ÄÅGPT-o1„ÄÅGemini-2.5„ÄÅGemini-2.0-flash-thinking„ÄÅGemini-1.5„ÄÅClaude-3.7-Sonnet„ÄÅClaude-3.5-Sonnet„ÄÅMistral-Large, etc.

Taking GPT-4o as an example, set the following key in the environment variables

```bash
export OPENAI_MODEL=xxxxxxxxx
export OPENAI_API_KEY=xxxxxxxxx
export OPENAI_BASE_URL=xxxxxxxxx
```

If using AZURE, set the following key:

```bash
export AZURE_OPENAI_DEPLOYMENT=xxxxxxxxx
export AZURE_OPENAI_ENDPOINT=xxxxxxxxx
export AZURE_OPENAI_API_KEY=xxxxxxxxx
export AZURE_OPENAI_API_VERSION=xxxxxxxxx
```

Afterwards, use the following code to request model results, setting the model to gpt4o. If the test is unexpectedly interrupted midway, you can modify the continue_file to continue the test. This will ensure that the already predicted results will not be predicted again.

ua format
```bash
cd c3_bench/bench_test

python3 request_pipeline_upta.py \
    --model=gpt4o \
    --data_path=./data/C3-Bench.jsonl \
    --output_path=./result \
    --language=en \
    --continue_file=empty.jsonl \
    --remove_role=True \
    --contain_context=True
```

upta format
```bash
cd c3_bench/bench_test

python3 request_pipeline.py \
    --model=gpt4o \
    --data_path=./data/C3-Bench.jsonl \
    --output_path=./result \
    --language=en \
    --continue_file=empty.jsonl \
    --remove_role=True \
    --contain_context=True
```

### ü§ó HuggingFace Models

This project also supports a variety of open-source specialized models and open-source general models, as follows:

Open-source specialized models include: xLAM2 series„ÄÅwatt-tool series„ÄÅToolACE2-8B„ÄÅToolACE-8B„ÄÅHammer2.1 series„ÄÅxLAM-7b-fc-r-7b„ÄÅgorilla-openfunctions-v2.

Open-source general models include: Qwen3 series, Qwen2.5 series, Llama-3.3 series„ÄÅGLM-4-9B-Chat, DeepSeek-R1, DeepSeek-V3.

Taking [Qwen2.5-7B-Instruct](https://qwen.readthedocs.io/en/latest/framework/function_call.html) as an example:  

First, you need to download the model to a specific directory, and then replace the directory path into the `tool_model_path_map` variable in `c3_bench/tool_calls/tool_model_map.py`.  

After that, you can deploy the model using the following code.

```bash
python3 web_server.py qwen7b
```

Afterward, use the following code to request the model results, set the model to qwen7b, and set the model_url to the IP and port number of your deployment machine, for example: http://111.111.111.111:12345. If the test stops unexpectedly, you can modify the continue_file to continue testing.

upta format
```bash
python3 request_pipeline_upta.py \
    --model=qwen7b \
    --data_path=./data/C3-Bench.jsonl \
    --output_path=./result \
    --language=en \
    --model_url=MODEL_URL \
    --continue_file=empty.jsonl \
    --remove_role=True \
    --contain_context=True
```

ua format
```bash
python3 request_pipeline.py \
    --model=qwen7b \
    --data_path=./data/C3-Bench.jsonl \
    --output_path=./result \
    --language=en \
    --model_url=MODEL_URL \
    --continue_file=empty.jsonl \
    --remove_role=True \
    --contain_context=True
```

Finally, in c3_bench/bench_test/handle/handles.py, we have enumerated the 10 types of Handles that we have implemented. If you want to test other models, you can refer to this file to get the settings for the model parameters. Additionally, if you want to add your own implemented Handle, you can also do so in this file.

## üí´ Evaluation

Use the following code to evaluate the model's prediction results. Fill in PREDICT_DATA_FILE with the corresponding prediction file from the previous step's ./result directory. The evaluation results include: matrix accuracy for action type and layer, individual accuracy for action type and layer, multi-tool invocation result analysis, error type analysis, true/false multi-turn accuracy, true multi-turn subtype accuracy, and parameter error type analysis.

Detailed results will be output to data_with_details.csv.

```bash
cd c3_bench/bench_test

python3 analysis_result.py \
    --data_file PREDICT_DATA_FILE \
    --output_csv_flag=True \
    --output_csv_path=./data_with_details.csv
```

In particular, we have persistently saved the results of all models reported in the paper in the c3_bench/bench_test/result directory, making it convenient for everyone to directly reproduce the results presented in the paper. This facilitates people developing Agent models to analyze bad cases, and provides a convenient way for people who want to quickly learn evaluation code, without the need to perform inference again.

Below is an example of how to reproduce the results of GPT-o1.

```bash
python3 analysis_result.py \
    --data_file ./result/2025-02-11-11:45:51_a5be8b_gpt4o1_en_remove_role_contain_context.jsonl \
    --output_csv_flag=True \
    --output_csv_path=./data_with_details.csv
```

Additionally, we have also supported the evaluation of prediction results from multiple models at the same time, which further enhances usability.

Below is an example of simultaneously reproducing the results of GPT-o1 and GPT-4o, with multiple files concatenated using a comma.

```bash
python3 analysis_result.py \
    --data_file ./result/2025-02-11-11:45:51_a5be8b_gpt4o1_en_remove_role_contain_context.jsonl,./result/2025-02-11-14:40:24_5ef3f9_gpt4o1120_en_remove_role_contain_context.jsonl \
    --output_csv_flag=True \
    --output_csv_path=./data_with_details.csv
```

## üß† Controllable Multi Agent Data Generation Framework

![MultiAgent](./picture/multi_agent2.png)

Our Paper designs a controllable multi agent data generation framework, which has the following eight unique advantages compared to other frameworks:

- **Controllable task Generation**: When generating tasks for each round, it is possible to control and specify the type of task currently needed, including single tool invocation, multiple tool invocations, tool invocation after clarification, and chat. It is this advantage that allows our framework to traverse all possible action spaces and construct unbiased data, which is very important in the field of large Language models. Whether for training or testing, the unbiased nature of the data directly determines whether the model's performance is excellent and whether the evaluation is reliable.
- **Specified Quantity task Generation**: Our framework can generate a specified number of tasks. Paired with the first advantage of controllable task generation, the generated data can cover all possible action spaces for any number of tasks.
- **Diversified task Generation**: In the first round of task generation, our framework can generate multiple tasks with different tones, lengths, themes/instances, scenarios, and role identities, and randomly select one to continue generating, offering extremely high diversity, close to the real distribution of humans.
- **True Multi-Turn task Generation**: In subsequent rounds of task generation, our framework is currently the only one that can controllably generate true multi-turn tasks. We can generate three core types of true multi-turn tasks, including implicit, ellipsis, and long-term memory. We also provide dozens of few-shot examples to guide the model in generating true multi-turn tasks, randomly selecting one of the examples each time, greatly enhancing data diversity and generation efficiency.
- **Rich Agents**: We have designed five major types of agents, including User agents, AI agents, Planner agents, Tool agents, and Checker agents, with a total of 15 subtypes. The diversity of agents ensures the diversity and high quality of the data generated by our framework.
- **Powerful Planner**: The Planner Agent we designed is currently the only agent in all intelligent agent frameworks that can make decisions on complex serial and parallel multi-tool invocation tasks. We have written prompts of over 4000 characters to guide it in making decisions according to our set guidelines, achieving a very high decision accuracy rate.
- **Reliable Checker**: The Checker Agent we designed is currently the only agent that checks the logic of parallel invocations. We have also written dozens of rules to check for low-level errors that the Planner might make and provide feedback, allowing it to reflect. Eventually, our Planner Agent and Checker Agent are used in combination, achieving a decision accuracy rate of over 90% without human intervention, which, to our knowledge, is the highest among all multi agent data generation frameworks.
- **Arbitrary Model Specification**: Our framework can use any LLM as the base model for the agents, allowing researchers to use any model they consider stronger to achieve better results.
- **Bilingual Support**: Our framework supports both English and Chinese, capable of generating data in both languages. To our knowledge, this is also the only framework currently supporting bilingual data generation.

![AgentFamily](./picture/agent_family.png)

### ‚ö°Ô∏è Quickstart

Taking the example where all agents use AZURE GPT-4o as the base model, and generate data in English. First, set the following key in the environment variables.

```bash
export AZURE_OPENAI_DEPLOYMENT=xxxxxxxxx
export AZURE_OPENAI_ENDPOINT=xxxxxxxxx
export AZURE_OPENAI_API_KEY=xxxxxxxxx
export AZURE_OPENAI_API_VERSION=xxxxxxxxx
export LANGUAGE=en
```

The core innovation of this paper lies in the fact that our proposed C3-Bench is capable of covering all possible action spaces for any number of tasks, and except for the first round, all are true multi-turn tasks. Therefore, our framework can support the generation of data for any number of tasks. Here we take the generation of four tasks as an example, with the reference code as follows:
```bash
cd multi_agent

python3 generate.py \
    --layer_num_total 4 \
    --user_model ["gpt4o"] \
    --planner_model "gpt4o" \
    --tool_model "gpt4o" \
    --agent_model "gpt4o" \
    --checker_model "gpt4o"
```
In particular, we have preserved data in three formats, including: raw generated data, streamlined data (with checker messages and role-switch messages removed), and captured data from each agent, available for researchers in need.

Finally, let's showcase a portion of the results generated for an actual case. It can be observed that the Continue Question User Agent has generated a true multi-turn task of implicit type, "Who is the uploader of the third wallpaper?" This implies that in order to solve this task, the model needs to refer to the preceding context to obtain the ID 778899 of the third wallpaper, and then continue to invoke the tool getWaifuDetails to retrieve detailed information about the wallpaper.

```
User: I need the latest 5 anime wallpapers.

PlannerÔºögetWaifuImages(limit=5, tags=["anime wallpapers"])

ToolÔºö...

PlannerÔºöprepare_to_answer(answer_type=tool)

AI:

Here are the latest 5 anime wallpapers:
1. Wallpaper ID: 112233
 - [View Wallpaper](https://wallhaven.cc/w/112233)
 - ![Thumbnail](https://wallhaven.cc/thumb/small/112233.jpg)
 - Resolution: 1920x1080
2. Wallpaper ID: 445566
 - [View Wallpaper](https://wallhaven.cc/w/445566)
 - ![Thumbnail](https://wallhaven.cc/thumb/small/445566.jpg)
 - Resolution: 2560x1440
3. Wallpaper ID: 778899
 - [View Wallpaper](https://wallhaven.cc/w/778899)
 - ![Thumbnail](https://wallhaven.cc/thumb/small/778899.jpg)
 - Resolution: 3840x2160
4. Wallpaper ID: 101112
 - [View Wallpaper](https://wallhaven.cc/w/101112)
 - ![Thumbnail](https://wallhaven.cc/thumb/small/101112.jpg)
 - Resolution: 1920x1080
5. Wallpaper ID: 131415
 - [View Wallpaper](https://wallhaven.cc/w/131415)
 - ![Thumbnail](https://wallhaven.cc/thumb/small/131415.jpg)
 - Resolution: 2560x1440

You can click on the links to view the full wallpapers. Hope you enjoy these wallpapers! If you need more help, please let me know.

Continue Question User Agent Generate:
User: Who is the uploader of the third wallpaper?

PlannerÔºögetWaifuDetails(image_id=778899)
```

It should be noted that even though our framework is capable of generating such excellent true multi-turn tasks, the generation of true multi-turn tasks remains very challenging for LLMs. Therefore, as we mentioned earlier, it was through the manual annotation by multiple experts that the accuracy was increased from less than 60% to 100%. This also includes modifying the pseudo multi-turn tasks generated by LLMs into true multi-turn tasks.
